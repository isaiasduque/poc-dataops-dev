steps:

- name: 'gcr.io/cloud-builders/gcloud'
  id: Copy Resources
  entrypoint: /bin/sh
  args:
  - '-c'
  - |
    gcloud config set project my-project-qa-421323 && \
    gsutil -m cp -r dataproc/jobs/*.py gs://datalake-raw-poc-qa/resources/dataproc/ && \
    gsutil -m cp -r schemas/*.sql gs://datalake-raw-poc-qa/resources/bigquery/

- name: 'gcr.io/cloud-builders/gcloud'
  id: Create schemas
  entrypoint: /bin/sh
  args:
  - '-c'
  - |
    gcloud config set project my-project-qa-421323 && \
    bq query --use_legacy_sql=false "$(gsutil cat gs://datalake-raw-poc-qa/resources/bigquery/00_schemas.sql)" && \
    bq query --use_legacy_sql=false "$(gsutil cat gs://datalake-raw-poc-qa/resources/bigquery/01_tbl_passengers.sql)" && \
    bq query --use_legacy_sql=false "$(gsutil cat gs://datalake-raw-poc-qa/resources/bigquery/02_tbl_passengers.sql)" && \
    bq query --use_legacy_sql=false "$(gsutil cat gs://datalake-raw-poc-qa/resources/bigquery/03_tbl_passengers.sql)"

- name: 'gcr.io/cloud-builders/gcloud'
  id: Create dataproc ephemeral workflow 
  entrypoint: /bin/sh
  args:
  - '-c'
  - |    
    gcloud dataproc workflow-templates import process_pyspark_etl \
    --source=dataproc/templates/job_pyspark.yaml --region=us-east4 && \
    gcloud dataproc workflow-templates set-managed-cluster process_pyspark_etl \
    --cluster-name=process-pyspark-etl \
    --single-node \
    --region=us-east4

options:
  logging: CLOUD_LOGGING_ONLY